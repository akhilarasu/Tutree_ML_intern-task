{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                  Tutree Machine Learning internship task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Explain dimension reduction in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction in Machine Learning\n",
    "\n",
    "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. When handling high dimensional data, it's often useful to scale back the dimensionality by projecting the info to a lower dimensional subspace which captures the “essence” of the info . This is called dimensionality reduction. High-dimensionality might mean hundreds, thousands, or even millions of input variables. Fewer input dimensions often mean correspondingly fewer parameters or an easier structure within the machine learning model, mentioned as degrees of freedom. A model with too many degrees of freedom is likely to overfit the training dataset and therefore may not perform well on new data. It is desirable to possess simple models that generalize well, and successively , input file with few input variables. This is particularly true for linear models where the number of inputs and the degrees of freedom of the model are often closely related. The fundamental reason for the curse of dimensionality is that high-dimensional functions have the potential to be much more complicated than low-dimensional ones, and that those complications are harder to discern. The only thanks to beat the curse is to include knowledge about the info that's correct.\n",
    "Dimensionality reduction may be a data preparation technique performed on data before modeling. It might be performed after data cleaning and data scaling and before training a predictive model. Dimensionality reduction yields a more compact, more easily interpretable representation of the target concept, focusing the user’s attention on the foremost relevant variables. As such, any dimensionality reduction performed on training data must also be performed on new data, such as a test dataset, validation dataset, and data when making a prediction with the final model.\n",
    "\n",
    "# Techniques for Dimensionality Reduction\n",
    "\n",
    "There are many techniques that can be used for dimensionality reduction. Some of them are discussed below,\n",
    "\n",
    "# 1. Feature Selection Methods\n",
    "\n",
    "Perhaps the most common are so-called feature selection techniques that use scoring or statistical methods to select which features to keep and which features to delete.\n",
    "Two main classes of feature selection techniques include wrapper methods and filter methods.\n",
    "\n",
    "# Wrapper methods \n",
    "as the name suggests, wrap a machine learning model, fitting and evaluating the model with different subsets of input features and selecting the subset the results in the best model performance. RFE is an example of a wrapper feature selection method.\n",
    "# Filter methods \n",
    "use scoring methods, like correlation between the feature and the target variable, to select a subset of input features that are most predictive. Examples include Pearson’s correlation and Chi-Squared test.\n",
    "Matrix Factorization\n",
    "Techniques from algebra are often used for dimensionality reduction. Specifically, matrix factorization methods can be used to reduce a dataset matrix into its constituent parts. Examples include the eigendecomposition and singular value decomposition. The parts can then be ranked and a subset of those parts can be selected that best captures the salient structure of the matrix that can be used to represent the dataset.\n",
    "The most common method for ranking the components is principal components analysis, or PCA for brief .\n",
    "The most common approach to dimensionality reduction is named principal components analysis or PCA.\n",
    "\n",
    "# 2. Manifold Learning\n",
    "Techniques from high-dimensionality statistics can also be used for dimensionality reduction. In mathematics, a projection is a kind of function or mapping that transforms data in some way. These techniques are sometimes referred to as “manifold learning” and are used to create a low-dimensional projection of high-dimensional data, often for the purposes of data visualization. The projection is designed to both create a low-dimensional representation of the dataset whilst best preserving the salient structure or relationships in the data.\n",
    "Examples of manifold learning techniques include:\n",
    "* Kohonen Self-Organizing Map (SOM).\n",
    "* Sammons Mapping\n",
    "* Multidimensional Scaling (MDS)\n",
    "* t-distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "# 3. Autoencoder Methods\n",
    "Deep learning neural networks can be constructed to perform dimensionality reduction.A popular approach is called autoencoders. This involves framing a self-supervised learning problem where a model must reproduce the input correctly. A network model is used that seeks to compress the data flow to a bottleneck layer with far fewer dimensions than the original input data. The part of the model prior to and including the bottleneck is referred to as the encoder, and the part of the model that reads the bottleneck output and reconstructs the input is called the decoder.\n",
    "An auto-encoder is a kind of unsupervised neural network that is used for dimensionality reduction and feature discovery. More precisely, an auto-encoder is a feedforward neural network that is trained to predict the input itself. After training, the decoder is discarded and the output from the bottleneck is used directly as the reduced dimensionality of the input. Inputs transformed by this encoder can then be fed into another model, not necessarily a neural network model. Deep autoencoders are an efficient framework for nonlinear dimensionality reduction. Once such a network has been built, the top-most layer of the encoder, the code layer hc, are often input to a supervised classification procedure. The output of the encoder is a type of projection, and like other projection methods, there is no direct relationship to the bottleneck output back to the original input variables, making them challenging to interpret.\n",
    "\n",
    "# An Example of Dimensionality Reduction: Email Classification using PCA\n",
    "Let’s set up a specific example to illustrate how PCA works. Assume that you simply have a database of emails and you would like to classify (using some machine learning numerical algorithm) each email as spam/not spam. To achieve this goal, you construct a mathematical representation of every email as a bag-of-words vector. This is a binary vector, where each position corresponds to a selected word from an alphabet. For an email, each entry in the bag-of-words vector is the number of times a corresponding word appears in an email (0 if it does not appear at all). Assume you have constructed a bag-of-words from each email, and as a result you have a sample of bag-of-words vectors x1…. xm. However, not all dimensions (words) of your vectors are informative for the spam/not spam classification. For instance, words “lottery”, “credit”, “pay” would be better features for spam classification than “dog”, “cat”, “tree”. For a mathematical thanks to reduce dimension we'll use PCA. For PCA you should construct an m-by-m covariance matrix from your sample x1…. Xm and compute its eigenvectors and eigenvalues. Next sort the resulting numbers during a decreasing order and choose p top eigenvalues. Applying PCA to your sample of vectors is projecting them onto eigenvectors corresponding to top p eigenvalues. Now, your output data is that the projection of original data onto p eigenvectors, the dimension of projected data has been reduced to p.\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) How can you handle duplicate values in a dataset for a variable in Python using pandas?\n",
    "Suppose, you are given the following dataset:\n",
    "df = pd.read_csv('file.csv')\n",
    "This dataset has many duplicate values. You need to identify them and also remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/Akhil/Desktop/file.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify duplicates records in the data\n",
    "\n",
    "dupes=df.duplicated()\n",
    "sum(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicates\n",
    "\n",
    "df_uniq=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
